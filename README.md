### Skeleton based action/activity recognition and abnormal behaviour detection methods

<details>
<summary>Open source softwares: (click to expand!)</summary>
1.[MMPose](https://github.com/open-mmlab/mmpose): an open-source toolbox for pose estimation based on PyTorch
2.[]():  
</details>
  
  
<details>
<summary>Paper list:</summary>
  
1.(2016) (RNN) [Deep LSTM + Part-Aware LSTM + NTU RGB+D dataset](https://openaccess.thecvf.com/content_cvpr_2016/html/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.html)

3.(2016) (RNN) [Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_50)

4.(2017) (RNN) [View Adaptive RNN for High Performance Human Action Recognition From Skeleton Data](https://openaccess.thecvf.com/content_iccv_2017/html/Zhang_View_Adaptive_Recurrent_ICCV_2017_paper.html)

5.(2017) (CNN) [Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/1705.08106)

6.(2017) (CNN) [Interpretable 3D Human Action Analysis With Temporal Convolutional Networks](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w20/html/Kim_Interpretable_3D_Human_CVPR_2017_paper.html)

7.(2017) (CNN) [Enhanced skeleton visualization for view invariant human action recognition](https://www.sciencedirect.com/science/article/pii/S0031320317300936)

8.(2018) (GCN) [Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1801.07455)

9.(2018) (GCN) [Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html)

10.(2019) (GCN) [Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.html)

11.(2019) (GCN) [Actional-Structural Graph Convolutional Networks for Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Actional-Structural_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.html)

12.(2019) (GCN) [An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content_CVPR_2019/html/Si_An_Attention_Enhanced_Graph_Convolutional_LSTM_Network_for_Skeleton-Based_Action_CVPR_2019_paper.html)

13.(2019) (GCN) [Skeleton-Based Action Recognition With Directed Graph Neural Networks](https://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html)

14.(2020) (GCN) [Skeleton-Based Action Recognition With Shift Graph Convolutional Network](https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html)

15.(2020) (GCN) [Context Aware Graph Convolution for Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Context_Aware_Graph_Convolution_for_Skeleton-Based_Action_Recognition_CVPR_2020_paper.html)

16.(2020) (GCN) [Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Semantics-Guided_Neural_Networks_for_Efficient_Skeleton-Based_Human_Action_Recognition_CVPR_2020_paper.html)
  
(2021) () [Revisiting Skeleton-based Action Recognition](https://arxiv.org/abs/2104.13586) [github](https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/README.md)
  
(2021) () [Memory Attention Networks for Skeleton-Based Action Recognition](https://ieeexplore.ieee.org/abstract/document/9378801)

(2021) (Transformer) [Spatial Temporal Transformer Network for Skeleton-Based Action Recognition](https://link.springer.com/chapter/10.1007/978-3-030-68796-0_50)

(2021) () [Quo Vadis, Skeleton Action Recognition?](https://link.springer.com/article/10.1007/s11263-021-01470-y)

(2021) (GCN) [Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction](https://ieeexplore.ieee.org/abstract/document/9334430/)

(2021) () [Tripool: Graph triplet pooling for 3D skeleton-based action recognition](https://www.sciencedirect.com/science/article/pii/S0031320321001084)

(2021) (GCN) [Spatial Temporal Graph Deconvolutional Network for Skeleton-Based Human Action Recognition](https://ieeexplore.ieee.org/abstract/document/9314910)

(2021) (GCN) [Skeleton-based action recognition using sparse spatio-temporal GCN with edge effective resistance](https://www.sciencedirect.com/science/article/pii/S0925231220317094)

(2021) (GCN) [Predictively encoded graph convolutional network for noise-robust skeleton-based action recognition](https://link.springer.com/article/10.1007/s10489-021-02487-z)

(2021) () [Structural Knowledge Distillation for Efficient Skeleton-Based Action Recognition](https://ieeexplore.ieee.org/abstract/document/9351789/)

(2021) () [Symmetrical Enhanced Fusion Network for Skeleton-based Action Recognition](https://ieeexplore.ieee.org/abstract/document/9319717)

(2021) (GCN) [Temporal Attention-Augmented Graph Convolutional Network for Efficient Skeleton-Based Human Action Recognition](https://ieeexplore.ieee.org/abstract/document/9412091)

(2021) () [Rethinking the ST-GCNs for 3D skeleton-based human action recognition](https://www.sciencedirect.com/science/article/pii/S0925231221007153)
  
</details>

<details>
<summary>Datasets:</summary>
  
### Skeleton-based Datasets:

(2013) [Human3.6M](https://ieeexplore.ieee.org/abstract/document/6682899), 2-dimensional space, 10 action classes, smaller than the next ones, human labelled.

(2016) [NTU RGB+D](https://openaccess.thecvf.com/content_cvpr_2016/html/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.html), 3-dimensional space, 60 action classes, large-scale, human labelled.

(2017) [Kinetics-Skeleton](https://arxiv.org/abs/1705.06950 ), 2-dimensional space, 400 action classes, large-scale, pose estimated, (original RGB videos).

(2019) [NTU-120 RGB+D](https://ieeexplore.ieee.org/abstract/document/8713892), 3-dimensional space 120 action classes, large scale, human labelled. 

### RGB video-based Datasets:

  (2014) [UCF Sports Action Data Set](https://www.crcv.ucf.edu/data/UCF_Sports_Action.php) - 150 clips with mean length of 6sec, 10 actions, resolution 720x480 

  (2019) [HOLLYWOOD2](https://www.di.ens.fr/~laptev/actions/hollywood2/) - 3669 video clips, 12 action classes and 10 classes of scenes, (20hours from 69 movies) 

  (2012) [UCF-101](https://arxiv.org/abs/1212.0402). 13,320 videos from youtube; 101 action classes, frame-level annotation.

  (2011) [HMDB-51](https://ieeexplore.ieee.org/document/6126543). 7000 videos from youtube, 51 action classes with at least 101 videos, frame-level annotation.

  (2017/2018/2020) [Kinetics-400](https://arxiv.org/abs/1705.06950), [Kinetics-600](https://arxiv.org/abs/1808.01340), [Kinetics-700](https://arxiv.org/abs/2010.10864). With 400 (/600/700) action classes with 400 (/600/700) 10-second videos for each class. Frame-level annotation.

  (2014) [Sports-1M](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html).  1,133,158 videos, automaticaly annotated for 487 actions at video-level. Each video has a url to be downloaded from.
  
  (2014) [THUMOS-14](https://www.sciencedirect.com/science/article/pii/S1077314216301710). 18000 videos, 101 action classes. With trimmed training videos and untrimmed test data, and with rame-level annotation.
  
  (2012) [UCF-101-24](https://arxiv.org/abs/1212.0402). Originated from the UCF-101 dataset with 24 action classes annotated at the pixel-level. 
  
  (2013) [J-HMDB-21](https://www.cv-foundation.org/openaccess/content_iccv_2013/html/Jhuang_Towards_Understanding_Action_2013_ICCV_paper.html). Originated from the HMDB-51 dataset with 21 action classes annotated at the pixel-level.
  
  (2018) [AVA](https://research.google.com/ava/), [well-written paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf). 430 video clips from movies, 15 minutes each with 900 keyframes, in each of which, the persons were labeled with multiple actions. With bounding box annotation.
  
  (2016/2019) [NTU RGB+D](https://openaccess.thecvf.com/content_cvpr_2016/html/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.html), [NTU RGB+D 120](https://ieeexplore.ieee.org/abstract/document/8713892/). 56,880 (/57,600) videos with 60 (/120) action classes that provides 3D skeleton and RGB-D data. bounding box annotations
</details>
